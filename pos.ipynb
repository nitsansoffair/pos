{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitsansoffair/pos/blob/master/pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15s6H-ArFR65"
      },
      "source": [
        "# parts of speech tagging"
      ],
      "id": "15s6H-ArFR65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w18Ein1_FR7E"
      },
      "outputs": [],
      "source": [
        "from utils_pos import get_word_tag, preprocess  \n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from math import log\n",
        "import numpy as np\n",
        "import w2_unittest"
      ],
      "id": "w18Ein1_FR7E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP0_FFnsFR7H"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/WSJ_02-21.pos\", 'r') as f:\n",
        "    training_corpus = f.readlines()"
      ],
      "id": "pP0_FFnsFR7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF6ed8SAFR7I"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/hmm_vocab.txt\", 'r') as f:\n",
        "    voc_l = f.read().split('\\n')"
      ],
      "id": "TF6ed8SAFR7I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5xYwggDFR7J"
      },
      "outputs": [],
      "source": [
        "vocab = {}\n",
        "for i, word in enumerate(sorted(voc_l)): \n",
        "    vocab[word] = i "
      ],
      "id": "D5xYwggDFR7J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXDnOhMKFR7L"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/WSJ_24.pos\", 'r') as f:\n",
        "    y = f.readlines()"
      ],
      "id": "EXDnOhMKFR7L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqiexvSoFR7N",
        "outputId": "62846219-6f54-4d25-d0d2-40fb88ed9e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the preprocessed test corpus:  34199\n",
            "This is a sample of the test_corpus: \n",
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ]
        }
      ],
      "source": [
        "_, prep = preprocess(vocab, \"./data/test.words\")     \n",
        "\n",
        "print('The length of the preprocessed test corpus: ', len(prep))\n",
        "print('This is a sample of the test_corpus: ')\n",
        "print(prep[0:10])"
      ],
      "id": "eqiexvSoFR7N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZztXtCFR7Q"
      },
      "source": [
        "#### Create dictionaries procedure."
      ],
      "id": "qAZztXtCFR7Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI7QQodSFR7R"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(training_corpus, vocab, verbose=False):\n",
        "    emission_counts, transition_counts, tag_counts = defaultdict(int), defaultdict(int), defaultdict(int)\n",
        "    prev_tag = '--s--'\n",
        "    i = 0\n",
        "    for word_tag in training_corpus:\n",
        "        i += 1\n",
        "        if i % 50000 == 0 and verbose:\n",
        "            print(f\"word count = {i}\")\n",
        "        word, tag = get_word_tag(word_tag, vocab)\n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        emission_counts[(tag, word)] += 1\n",
        "        tag_counts[tag] += 1\n",
        "        prev_tag = tag\n",
        "    return emission_counts, transition_counts, tag_counts"
      ],
      "id": "FI7QQodSFR7R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaZ-UoF3FR7T"
      },
      "outputs": [],
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
      ],
      "id": "yaZ-UoF3FR7T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUN4xaJvFR7W",
        "outputId": "adb8f9a5-608b-4e78-c59f-a219c1fe3048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of POS tags (number of 'states'): 46\n",
            "View these POS tags (states)\n",
            "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ],
      "source": [
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ],
      "id": "GUN4xaJvFR7W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35LczwFVFR7X",
        "outputId": "0ac9e3e5-f7d5-4811-e4b2-fb13ea8c4ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transition examples: \n",
            "(('--s--', 'IN'), 5050)\n",
            "(('IN', 'DT'), 32364)\n",
            "(('DT', 'NNP'), 9044)\n",
            "\n",
            "emission examples: \n",
            "(('DT', 'any'), 721)\n",
            "(('NN', 'decrease'), 7)\n",
            "(('NN', 'insider-trading'), 5)\n",
            "\n",
            "ambiguous word example: \n",
            "('RB', 'back') 304\n",
            "('VB', 'back') 20\n",
            "('RP', 'back') 84\n",
            "('JJ', 'back') 25\n",
            "('NN', 'back') 29\n",
            "('VBP', 'back') 4\n"
          ]
        }
      ],
      "source": [
        "print(\"transition examples: \")\n",
        "for ex in list(transition_counts.items())[:3]:\n",
        "    print(ex)\n",
        "print()\n",
        "\n",
        "print(\"emission examples: \")\n",
        "for ex in list(emission_counts.items())[200:203]:\n",
        "    print (ex)\n",
        "print()\n",
        "\n",
        "print(\"ambiguous word example: \")\n",
        "for tup,cnt in emission_counts.items():\n",
        "    if tup[1] == 'back': print (tup, cnt) "
      ],
      "id": "35LczwFVFR7X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2NvLAgIFR7a"
      },
      "source": [
        "#### Predict pos of word procedure."
      ],
      "id": "N2NvLAgIFR7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVMoJhupFR7a"
      },
      "outputs": [],
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    num_correct, total = 0, len(y)\n",
        "    for word_tag, y_tup in zip(prep, y):\n",
        "        y_tup_l = y_tup.split()\n",
        "        if len(y_tup_l) == 2:\n",
        "            true_label = y_tup_l[1]\n",
        "        else:\n",
        "            continue\n",
        "        pos_final, count_final = '', 0\n",
        "        word = word_tag.split('\\t')[0]\n",
        "        if word in list(vocab.keys()):\n",
        "            for pos in states:\n",
        "                key = (pos, word)\n",
        "                if key in emission_counts.keys():\n",
        "                    count = emission_counts[key]\n",
        "                    if count > count_final:\n",
        "                        count_final, pos_final = count, key[0]\n",
        "            if pos_final == true_label:\n",
        "                num_correct += 1\n",
        "    return num_correct / total"
      ],
      "id": "lVMoJhupFR7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9usr1VSFR7c",
        "outputId": "330738b9-4c76-4640-c6d5-a8f5079f07a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of prediction using predict_pos is 0.8889\n"
          ]
        }
      ],
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ],
      "id": "N9usr1VSFR7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rUoZ78FR7f"
      },
      "source": [
        "#### Create transition matrix procedure."
      ],
      "id": "99rUoZ78FR7f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgq3JCe_FR7h"
      },
      "outputs": [],
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_tags):\n",
        "            count = 0\n",
        "            key = (all_tags[i], all_tags[j]) \n",
        "            if key in trans_keys:\n",
        "                count = transition_counts[key]\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "            A[i, j] = (count + alpha) / (count_prev_tag + num_tags * alpha)\n",
        "    return A"
      ],
      "id": "Qgq3JCe_FR7h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HsJag0eFR7j",
        "outputId": "d7830c9e-12ef-44e0-da32-e16a4d17065a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A at row 0, col 0: 0.000007040\n",
            "A at row 3, col 1: 0.1691\n",
            "View a subset of transition matrix A\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
            "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
            "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
            "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
            "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.001\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)"
      ],
      "id": "6HsJag0eFR7j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeoz17ZhFR7l"
      },
      "source": [
        "#### Create emission matrix procedure."
      ],
      "id": "Xeoz17ZhFR7l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQNiQQSpFR7l"
      },
      "outputs": [],
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    num_tags, num_words = len(tag_counts), len(vocab.keys())\n",
        "    emission_matrix = np.zeros((num_tags, num_words))\n",
        "    emis_keys, vocab_keys = list(emission_counts.keys()), list(vocab.keys())\n",
        "    for tag in range(num_tags):\n",
        "        sum_rows = 0\n",
        "        for word in range(num_words):\n",
        "            key = (list(tag_counts.keys())[tag], vocab_keys[word])\n",
        "            count = emission_counts[key] if key in emission_counts.keys() else 0\n",
        "            emission_matrix[tag, word] = count + alpha\n",
        "            sum_rows += emission_matrix[tag, word]\n",
        "        emission_matrix[tag, :] /= sum_rows\n",
        "    return emission_matrix"
      ],
      "id": "CQNiQQSpFR7l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKo2qQqwFR7m",
        "outputId": "45304328-7507-460f-8f83-4a845dc402ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View Matrix position at row 0, column 0: 0.000000010\n",
            "View Matrix position at row 3, column 1: 0.000000027\n",
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
            "NN   4.609192e-08  4.609192e-08  4.609192e-08  4.609192e-08  4.609192e-08\n",
            "NNS  1.186130e-07  1.186130e-07  1.186130e-07  1.186130e-07  1.186130e-07\n",
            "VB   4.615150e-07  4.615150e-07  4.615150e-07  4.615150e-07  4.615150e-07\n",
            "RB   5.581052e-07  5.581052e-07  5.581052e-07  5.581052e-07  5.581052e-07\n",
            "RP   2.316007e-07  2.316007e-07  2.316007e-07  2.316007e-07  2.316007e-07\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.001\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, vocab)\n",
        "\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "\n",
        "cols = [vocab[a] for a in cidx]\n",
        "\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "\n",
        "rows = [states.index(a) for a in rvals]\n",
        "\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ],
      "id": "wKo2qQqwFR7m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMN2BcfvFR7n"
      },
      "source": [
        "#### Viterbi algorithm Initialize procedure."
      ],
      "id": "pMN2BcfvFR7n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-5Y0JbOFR7o"
      },
      "outputs": [],
      "source": [
        "def initialize(states, tag_counts, A, B, corpus, vocab, start_token=\"--s--\"):\n",
        "    num_tags = len(tag_counts)\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    s_idx = states.index(start_token)\n",
        "    for tag in range(num_tags):\n",
        "        if A[0, tag] == 0:\n",
        "            best_probs[tag, 0] = float(\"-inf\")\n",
        "        else:\n",
        "            word = corpus[0].split('\\t')[0]\n",
        "            best_probs[tag, 0] = log(A[s_idx, tag] + B[tag, vocab[word]]) if A[s_idx, tag] != 0 else float('-inf')\n",
        "    return best_probs, best_paths"
      ],
      "id": "b-5Y0JbOFR7o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFWQx9aVFR7p"
      },
      "outputs": [],
      "source": [
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ],
      "id": "VFWQx9aVFR7p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpAsBTjQFR7p"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/WSJ_24.pos\", 'r') as f:\n",
        "    testing_corpus = f.readlines()"
      ],
      "id": "CpAsBTjQFR7p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qii9xDGuFR7p"
      },
      "source": [
        "#### Viterbi algorithm forward procedure."
      ],
      "id": "qii9xDGuFR7p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9FA0kVeFR7q"
      },
      "outputs": [],
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
        "    num_tags = best_probs.shape[0]\n",
        "    for word in range(1, len(vocab)):\n",
        "        if word % 5000 == 0 and verbose:\n",
        "            print(\"Words processed: {:>8}\".format(word))\n",
        "        for tag in range(num_tags):\n",
        "            best_prob, best_path = float(\"-inf\"), None\n",
        "            for previous_tag in range(num_tags):\n",
        "                prob = best_probs[previous_tag, word - 1] + log(A[previous_tag, tag]) + log(B[tag, word])\n",
        "                if prob > best_prob:\n",
        "                    best_prob = prob\n",
        "                    best_path = previous_tag\n",
        "            best_probs[tag, word] = best_prob\n",
        "            best_paths[tag, word] = best_path\n",
        "    return best_probs, best_paths"
      ],
      "id": "e9FA0kVeFR7q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcedLD7fFR7r",
        "outputId": "bc45ea60-0233-49f7-e3b4-ff4e06e3bd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words processed:     5000\n",
            "Words processed:    10000\n",
            "Words processed:    15000\n",
            "Words processed:    20000\n"
          ]
        }
      ],
      "source": [
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "id": "hcedLD7fFR7r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfL_ty_tFR7s"
      },
      "source": [
        "#### Viterbi algorithm backward procedure."
      ],
      "id": "HfL_ty_tFR7s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPkSpHGgFR7t"
      },
      "outputs": [],
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    last_word = best_paths.shape[1]\n",
        "    z = [None] * last_word\n",
        "    num_tags = best_probs.shape[0]\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    predictions = [None] * last_word\n",
        "    for tag in range(num_tags):\n",
        "        if best_probs[tag, last_word - 1] > best_prob_for_last_word:\n",
        "            best_prob_for_last_word, z[last_word - 1] = best_probs[tag, last_word - 1], tag\n",
        "    predictions[last_word - 1] = states[z[last_word - 1]]\n",
        "    for word in range(len(corpus) - 1, -1, -1):\n",
        "        tag_index = np.argmax(best_probs[:, word], axis=0)\n",
        "        pos_tag_for_word, z[word - 1] = states[tag_index], best_paths[tag_index, word]\n",
        "        predictions[word - 1] = pos_tag_for_word\n",
        "    return predictions"
      ],
      "id": "FPkSpHGgFR7t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7iWxx7kFR7u"
      },
      "source": [
        "#### Compute accuracy procedure."
      ],
      "id": "S7iWxx7kFR7u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEQCDBJjFR7u"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    num_correct, total = 0, 0\n",
        "    for prediction, true_label in zip(pred, y):\n",
        "        word_tag_tuple = true_label.split('\\t')\n",
        "        if len(word_tag_tuple) != 2:\n",
        "            continue\n",
        "        word, tag = word_tag_tuple\n",
        "        num_correct += 1 if tag[:-1] == prediction else 0\n",
        "        total += 1\n",
        "    return num_correct / total"
      ],
      "id": "sEQCDBJjFR7u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwJODTnEFR7v"
      },
      "source": [
        "### References\n",
        "\n",
        "- Coursera - Natural language processing with probabilistic models [course](https://www.coursera.org/learn/probabilistic-models-in-nlp)."
      ],
      "id": "wwJODTnEFR7v"
    }
  ],
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "pos.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}